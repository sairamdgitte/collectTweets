{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24c84cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install mysql-connector-python\n",
    "# !pip install nltk\n",
    "# # nltk.download('stopwords')\n",
    "# # nltk.download('wordnet')\n",
    "# # nltk.download('omw-1.4')\n",
    "# !pip install sqlalchemy\n",
    "# !pip install pymysql\n",
    "# !pip install --user --upgrade \"git+https://github.com/twintproject/twint.git@origin/master#egg=twint\"\n",
    "# !pip install plotly\n",
    "# !pip install chart_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3100819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca8c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f508390a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mysql.connector, twint\n",
    "import nltk, numpy as np \n",
    "import string\n",
    "from string import punctuation\n",
    "import re\n",
    "from mysql.connector import Error\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e4c8310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>population_proper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>-33.8650</td>\n",
       "      <td>151.2094</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>admin</td>\n",
       "      <td>5312163</td>\n",
       "      <td>4840600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>-37.8136</td>\n",
       "      <td>144.9631</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>admin</td>\n",
       "      <td>5078193</td>\n",
       "      <td>4529500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>-27.4678</td>\n",
       "      <td>153.0281</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>admin</td>\n",
       "      <td>2514184</td>\n",
       "      <td>2360241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perth</td>\n",
       "      <td>-31.9522</td>\n",
       "      <td>115.8589</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>admin</td>\n",
       "      <td>2059484</td>\n",
       "      <td>2039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>-34.9289</td>\n",
       "      <td>138.6011</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>admin</td>\n",
       "      <td>1345777</td>\n",
       "      <td>1295714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Bunyip</td>\n",
       "      <td>-38.0833</td>\n",
       "      <td>145.7170</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2468</td>\n",
       "      <td>2468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>Dodges Ferry</td>\n",
       "      <td>-42.8539</td>\n",
       "      <td>147.6194</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>Tasmania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2467</td>\n",
       "      <td>2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>Lancefield</td>\n",
       "      <td>-37.2667</td>\n",
       "      <td>144.7167</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2455</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>Palm Island</td>\n",
       "      <td>-18.7345</td>\n",
       "      <td>146.5794</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2455</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>Currie</td>\n",
       "      <td>-39.9311</td>\n",
       "      <td>143.8510</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AU</td>\n",
       "      <td>Tasmania</td>\n",
       "      <td>minor</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             city      lat       lng    country iso2         admin_name  \\\n",
       "0          Sydney -33.8650  151.2094  Australia   AU    New South Wales   \n",
       "1       Melbourne -37.8136  144.9631  Australia   AU           Victoria   \n",
       "2        Brisbane -27.4678  153.0281  Australia   AU         Queensland   \n",
       "3           Perth -31.9522  115.8589  Australia   AU  Western Australia   \n",
       "4        Adelaide -34.9289  138.6011  Australia   AU    South Australia   \n",
       "..            ...      ...       ...        ...  ...                ...   \n",
       "538        Bunyip -38.0833  145.7170  Australia   AU           Victoria   \n",
       "539  Dodges Ferry -42.8539  147.6194  Australia   AU           Tasmania   \n",
       "540    Lancefield -37.2667  144.7167  Australia   AU           Victoria   \n",
       "541   Palm Island -18.7345  146.5794  Australia   AU         Queensland   \n",
       "542        Currie -39.9311  143.8510  Australia   AU           Tasmania   \n",
       "\n",
       "    capital  population  population_proper  \n",
       "0     admin     5312163            4840600  \n",
       "1     admin     5078193            4529500  \n",
       "2     admin     2514184            2360241  \n",
       "3     admin     2059484            2039200  \n",
       "4     admin     1345777            1295714  \n",
       "..      ...         ...                ...  \n",
       "538     NaN        2468               2468  \n",
       "539     NaN        2467               2467  \n",
       "540     NaN        2455               2455  \n",
       "541     NaN        2455               2455  \n",
       "542   minor         768                768  \n",
       "\n",
       "[543 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = pd.read_csv('au.csv')\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ce23ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator object\n",
    "translated = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "# Sentiment analyzer object \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Porter stemmer\n",
    "# ps = nltk.PorterStemmer()\n",
    "\n",
    "# # Lemmatizer\n",
    "# wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Stopword removal\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Stopword removal\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a047344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "#Removing emojis, symbols, maps, flags etc\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        \n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove Emails\n",
    "    text =  re.sub('\\S*@\\S*\\s?', '', text) \n",
    "    \n",
    "    # Remove words with numbers and numbers\n",
    "    text = re.sub('\\w*[0-9]\\w*\\s?', '', text)\n",
    "    \n",
    "    # Remove #'s\n",
    "    text =  re.sub(\"[$&+,:;=?@#|'<>.^*()%!_-]?\", '', text)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    text = re.sub('[^a-zA-z0-9\\s]','',text)\n",
    "    text = re.sub('[0-9]','', text)\n",
    "\n",
    "    text = re.sub('rt', '', text)\n",
    "\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    text = re.split('\\W+', text)    # tokenization\n",
    "    \n",
    "    \n",
    "    # Stopword removal\n",
    "    text = [word for word in text if word not in stopword]  # remove stopwords and stemming\n",
    "    \n",
    "    \n",
    "#     # Remove non-english words / sentances\n",
    "#     text = \" \".join(w for w in text if w.lower() in words or not w.isalpha())\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # Strip the leading spaces\n",
    "    text = text.lstrip().rstrip()\n",
    "    \n",
    "    if len(text.split()) > 0:\n",
    "#         text = translated.translate(text)\n",
    "        return text\n",
    "    else:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86328c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6dfea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation(parm):\n",
    "    a = translated.translate_batch(parm)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1deca6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text1(text):\n",
    "#     text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "#     text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "#     tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "#     text = [wn.lemmatize(ps.stem(word)) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af263dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_server_connection(host_name, user_name, user_password):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "        host=host_name,\n",
    "        user=user_name,\n",
    "        passwd=user_password,\n",
    "        )\n",
    "        print(\"MySQL Database connection successul\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c881f680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL Database connection successul\n"
     ]
    }
   ],
   "source": [
    "connection = create_server_connection(\"localhost\", \"root\", \"#Sentiment2022analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2a00b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        print(\"Database created successfully\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe1f9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: '1007 (HY000): Can't create database 'htl_sentiment'; database exists'\n"
     ]
    }
   ],
   "source": [
    "create_database_sentiment = \"CREATE DATABASE HTL_SENTIMENT\"\n",
    "create_database(connection,create_database_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d69d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(host_name, user_name, user_password, db_name):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "        host=host_name,\n",
    "        user=user_name,\n",
    "        passwd=user_password,\n",
    "        database=db_name\n",
    "        )\n",
    "        print(\"MySQL Database connection successul\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1501d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor(buffered=True)\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        connection.commit()\n",
    "        print(\"Query successful\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "774a2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = \"#Sentiment2022analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33b16ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL Database connection successul\n"
     ]
    }
   ],
   "source": [
    "connection = create_db_connection(\"localhost\", \"root\", pw, \"HTL_SENTIMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc712866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM TWEET;\", con=connection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fbe4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.shape[0] == 0:\n",
    "    drop_table = \"\"\"\n",
    "    DROP TABLE tweet;\n",
    "    \"\"\"\n",
    "    execute_query(connection, drop_table)\n",
    "    \n",
    "    \n",
    "    create_tweet_table = \"\"\"\n",
    "    CREATE TABLE tweet (\n",
    "    id VARCHAR(100),\n",
    "    original_tweet MEDIUMTEXT NOT NULL,\n",
    "    tweet MEDIUMTEXT NOT NULL,\n",
    "    lat DECIMAL(8,5),\n",
    "    lng DECIMAL(8,5),\n",
    "    place VARCHAR(50),\n",
    "    date VARCHAR(50),\n",
    "    sentiment INT(50),\n",
    "    PRIMARY KEY (`id`,`date`)\n",
    "    );\n",
    "    \"\"\"\n",
    "    execute_query(connection, create_tweet_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb908e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>place</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1470219245141172224</td>\n",
       "      <td>@melblawyer26 @GiselleWak Perhaps if you had s...</td>\n",
       "      <td>perhaps studied medicine instead law comment w...</td>\n",
       "      <td>-27.6167</td>\n",
       "      <td>152.7667</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>2021-12-13 13:29:18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1470230849996181504</td>\n",
       "      <td>I'm at Essential Oils &amp;amp; Wellness with Fion...</td>\n",
       "      <td>im essential oils amp wellness fiona californi...</td>\n",
       "      <td>-36.7500</td>\n",
       "      <td>144.2667</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2021-12-13 14:15:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1470305021078081536</td>\n",
       "      <td>@susan_taylor07 Removing franking credits wasn...</td>\n",
       "      <td>removing franking credits wasnt scare campaign...</td>\n",
       "      <td>-27.6167</td>\n",
       "      <td>152.7667</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>2021-12-13 19:10:08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1470353065857351684</td>\n",
       "      <td>@Hateonions1 Go to your apps &amp;amp; click on th...</td>\n",
       "      <td>go apps amp click service nsw come update opti...</td>\n",
       "      <td>-27.6167</td>\n",
       "      <td>152.7667</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>2021-12-13 22:21:03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1470359518949167106</td>\n",
       "      <td>@Alannahjoynes @DamoMK @OptusStadium You can l...</td>\n",
       "      <td>link medicare account one state checkin apps r...</td>\n",
       "      <td>-27.6167</td>\n",
       "      <td>152.7667</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>2021-12-13 22:46:41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34875</th>\n",
       "      <td>1493982462103662594</td>\n",
       "      <td>Further correlation between Vaccines and death...</td>\n",
       "      <td>fuher correlation vaccines death rate good way</td>\n",
       "      <td>-34.4331</td>\n",
       "      <td>150.8831</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>2022-02-17 03:15:50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34876</th>\n",
       "      <td>1493982483272314883</td>\n",
       "      <td>#covid19 #vaccine by @AndyMarlette  https://t....</td>\n",
       "      <td>vaccine maga kag tcot uspoli</td>\n",
       "      <td>-34.4331</td>\n",
       "      <td>150.8831</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>2022-02-17 03:15:55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34877</th>\n",
       "      <td>1493982516012896260</td>\n",
       "      <td>@FromItsa @PinkWug @shoe0nhead @theserfstv Not...</td>\n",
       "      <td>said vaccines offer excellent protection vacci...</td>\n",
       "      <td>-34.4331</td>\n",
       "      <td>150.8831</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>2022-02-17 03:16:03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34878</th>\n",
       "      <td>1493998083012579329</td>\n",
       "      <td>@AnnRoll67123915 @FlavioVolpe1 I am sitting he...</td>\n",
       "      <td>sitting recovering covid fully vaccinated week...</td>\n",
       "      <td>-27.5667</td>\n",
       "      <td>151.9500</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>2022-02-17 04:17:54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34879</th>\n",
       "      <td>1494004452864323584</td>\n",
       "      <td>@alisonmah @GetBAC But we were told it's the p...</td>\n",
       "      <td>told protesters violent apparently seems love ...</td>\n",
       "      <td>-36.7500</td>\n",
       "      <td>144.2667</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-02-17 04:43:13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34880 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                     original_tweet  \\\n",
       "0      1470219245141172224  @melblawyer26 @GiselleWak Perhaps if you had s...   \n",
       "1      1470230849996181504  I'm at Essential Oils &amp; Wellness with Fion...   \n",
       "2      1470305021078081536  @susan_taylor07 Removing franking credits wasn...   \n",
       "3      1470353065857351684  @Hateonions1 Go to your apps &amp; click on th...   \n",
       "4      1470359518949167106  @Alannahjoynes @DamoMK @OptusStadium You can l...   \n",
       "...                    ...                                                ...   \n",
       "34875  1493982462103662594  Further correlation between Vaccines and death...   \n",
       "34876  1493982483272314883  #covid19 #vaccine by @AndyMarlette  https://t....   \n",
       "34877  1493982516012896260  @FromItsa @PinkWug @shoe0nhead @theserfstv Not...   \n",
       "34878  1493998083012579329  @AnnRoll67123915 @FlavioVolpe1 I am sitting he...   \n",
       "34879  1494004452864323584  @alisonmah @GetBAC But we were told it's the p...   \n",
       "\n",
       "                                                   tweet      lat       lng  \\\n",
       "0      perhaps studied medicine instead law comment w... -27.6167  152.7667   \n",
       "1      im essential oils amp wellness fiona californi... -36.7500  144.2667   \n",
       "2      removing franking credits wasnt scare campaign... -27.6167  152.7667   \n",
       "3      go apps amp click service nsw come update opti... -27.6167  152.7667   \n",
       "4      link medicare account one state checkin apps r... -27.6167  152.7667   \n",
       "...                                                  ...      ...       ...   \n",
       "34875     fuher correlation vaccines death rate good way -34.4331  150.8831   \n",
       "34876                       vaccine maga kag tcot uspoli -34.4331  150.8831   \n",
       "34877  said vaccines offer excellent protection vacci... -34.4331  150.8831   \n",
       "34878  sitting recovering covid fully vaccinated week... -27.5667  151.9500   \n",
       "34879  told protesters violent apparently seems love ... -36.7500  144.2667   \n",
       "\n",
       "                 place                 date  sentiment  \n",
       "0           Queensland  2021-12-13 13:29:18          0  \n",
       "1             Victoria  2021-12-13 14:15:24          0  \n",
       "2           Queensland  2021-12-13 19:10:08          0  \n",
       "3           Queensland  2021-12-13 22:21:03          0  \n",
       "4           Queensland  2021-12-13 22:46:41          0  \n",
       "...                ...                  ...        ...  \n",
       "34875  New South Wales  2022-02-17 03:15:50          0  \n",
       "34876  New South Wales  2022-02-17 03:15:55          0  \n",
       "34877  New South Wales  2022-02-17 03:16:03          1  \n",
       "34878       Queensland  2022-02-17 04:17:54          0  \n",
       "34879         Victoria  2022-02-17 04:43:13          0  \n",
       "\n",
       "[34880 rows x 8 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c407bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+pymysql://root:#Sentiment2022analysis@localhost/HTL_SENTIMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "134d61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.dialects.mysql import insert\n",
    "\n",
    "def insert_on_duplicate(table, conn, keys, data_iter):\n",
    "    insert_stmt = insert(table.table).values(list(data_iter))\n",
    "    on_duplicate_key_stmt = insert_stmt.on_duplicate_key_update(insert_stmt.inserted)\n",
    "    conn.execute(on_duplicate_key_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcce66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2b4974d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-02-17 04:43:13'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756f940",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad601b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c042abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['medicare', 'health', 'omicron', 'medical', 'wellness', 'medicine', 'vaccine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29528b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sydney\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:sqlalchemy.pool.impl.QueuePool:Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sai Ram\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py\", line 756, in _write_bytes\n",
      "    self._sock.sendall(data)\n",
      "ConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sai Ram\\Anaconda3\\lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 739, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"C:\\Users\\Sai Ram\\Anaconda3\\lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 988, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"C:\\Users\\Sai Ram\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\default.py\", line 669, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "  File \"C:\\Users\\Sai Ram\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py\", line 479, in rollback\n",
      "    self._execute_command(COMMAND.COM_QUERY, \"ROLLBACK\")\n",
      "  File \"C:\\Users\\Sai Ram\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py\", line 814, in _execute_command\n",
      "    self._write_bytes(packet)\n",
      "  File \"C:\\Users\\Sai Ram\\Anaconda3\\lib\\site-packages\\pymysql\\connections.py\", line 760, in _write_bytes\n",
      "    CR.CR_SERVER_GONE_ERROR, \"MySQL server has gone away (%r)\" % (e,)\n",
      "pymysql.err.OperationalError: (2006, \"MySQL server has gone away (ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Brisbane\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Perth\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Adelaide\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Gold Coast\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 53 Tweets.\n",
      "Cranbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Canberra\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Newcastle\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 60 Tweets.\n",
      "Wollongong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Geelong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Hobart\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 10 Tweets.\n",
      "Townsville\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 16 Tweets.\n",
      "Ipswich\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Cairns\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 13 Tweets.\n",
      "Toowoomba\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 53 Tweets.\n",
      "Darwin\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 29 Tweets.\n",
      "Ballarat\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Bendigo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Launceston\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 10 Tweets.\n",
      "Sydney\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Melbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Brisbane\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 80 Tweets.\n",
      "Perth\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Adelaide\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Gold Coast\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 80 Tweets.\n",
      "Cranbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Canberra\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Newcastle\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Wollongong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 80 Tweets.\n",
      "Geelong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Hobart\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Townsville\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ipswich\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Cairns\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 112 Tweets.\n",
      "Toowoomba\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Darwin\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 23 Tweets.\n",
      "Ballarat\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Bendigo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Launceston\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Sydney\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Melbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Brisbane\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Perth\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Adelaide\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Gold Coast\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Cranbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Canberra\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Newcastle\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Wollongong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Geelong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Hobart\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 60 Tweets.\n",
      "Townsville\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ipswich\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Cairns\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Toowoomba\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Darwin\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ballarat\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Bendigo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Launceston\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Sydney\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Brisbane\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 39 Tweets.\n",
      "Perth\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 59 Tweets.\n",
      "Adelaide\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 60 Tweets.\n",
      "Gold Coast\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 54 Tweets.\n",
      "Cranbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 60 Tweets.\n",
      "Canberra\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Newcastle\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Wollongong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Geelong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Hobart\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Townsville\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ipswich\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 19 Tweets.\n",
      "Cairns\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Toowoomba\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Darwin\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ballarat\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Bendigo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Launceston\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Sydney\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 15 Tweets.\n",
      "Melbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Brisbane\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Perth\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Adelaide\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 6 Tweets.\n",
      "Gold Coast\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Cranbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 21 Tweets.\n",
      "Canberra\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 10 Tweets.\n",
      "Newcastle\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Wollongong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Geelong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Hobart\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Townsville\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ipswich\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Cairns\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Toowoomba\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Darwin\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ballarat\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Bendigo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 21 Tweets.\n",
      "Launceston\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 9 Tweets.\n",
      "Sydney\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Melbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 19 Tweets.\n",
      "Brisbane\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Perth\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Adelaide\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 12 Tweets.\n",
      "Gold Coast\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 19 Tweets.\n",
      "Cranbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 35 Tweets.\n",
      "Canberra\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 40 Tweets.\n",
      "Newcastle\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Wollongong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 60 Tweets.\n",
      "Geelong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Hobart\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Townsville\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ipswich\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Cairns\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 5 Tweets.\n",
      "Toowoomba\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Darwin\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ballarat\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Bendigo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 19 Tweets.\n",
      "Launceston\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Sydney\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 57 Tweets.\n",
      "Brisbane\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Perth\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Adelaide\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Gold Coast\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 66 Tweets.\n",
      "Cranbourne\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 37 Tweets.\n",
      "Canberra\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 97 Tweets.\n",
      "Newcastle\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Wollongong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 19 Tweets.\n",
      "Geelong\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Hobart\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Townsville\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Ipswich\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 46 Tweets.\n",
      "Cairns\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Toowoomba\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 66 Tweets.\n",
      "Darwin\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 20 Tweets.\n",
      "Ballarat\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Bendigo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n",
      "Launceston\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 0 Tweets.\n"
     ]
    }
   ],
   "source": [
    "for i in keywords:\n",
    "    \n",
    "    for j in cities.city.values[0:20].tolist():\n",
    "        print(j)\n",
    "        c = twint.Config()\n",
    "\n",
    "        c.Search = i\n",
    "    #     c.Seacrh = \"medicare OR health OR omicron OR medical OR wellness OR medicine OR vaccine\"\n",
    "    #     c.Search = \"health\" \n",
    "        c.Geo = '{},{},300km'.format(cities[cities['city']==j].iloc[0,1],cities[cities['city']==j].iloc[0,2])\n",
    "        c.Lang = 'en'\n",
    "        c.Store_object = True\n",
    "        c.Count = True\n",
    "        c.Hide_output = True\n",
    "        c.Since = '2021-12-17'\n",
    "        c.Until = '2022-02-05'\n",
    "#         c.Until = '2022-01-09'\n",
    "        c.Filter_retweets = True\n",
    "        c.Pandas = True\n",
    "    #     c.Limit = 10000\n",
    "\n",
    "        twint.run.Search(c)\n",
    "\n",
    "        # Sleep time added\n",
    "        time.sleep(2)\n",
    "\n",
    "        twint.storage.panda.Tweets_df.place = cities[cities['city']==j].iloc[0,5] \n",
    "\n",
    "    #     df = pd.concat([df, twint.storage.panda.Tweets_df])\n",
    "        df_twint = twint.storage.panda.Tweets_df.copy()\n",
    "        df_twint['lat'] = cities[cities['city']==j].iloc[0,1]\n",
    "        df_twint['lng'] = cities[cities['city']==j].iloc[0,2]\n",
    "\n",
    "        if df_twint.shape[0] > 0:\n",
    "            df_twint = df_twint.drop_duplicates('tweet')\n",
    "\n",
    "\n",
    "            df_twint = df_twint[[\"id\", \"tweet\", \"lat\", \"lng\", \"place\", \"date\"]]\n",
    "\n",
    "            df_twint['sentiment_tweet'] = df_twint.tweet.apply(lambda x: clean_text(x))\n",
    "            df_twint = df_twint.dropna(subset=['sentiment_tweet'])\n",
    "            df_twint['sentiment_tweet'] = translation(df_twint.sentiment_tweet.values.tolist())\n",
    "\n",
    "            df_twint['sentiment'] = df_twint['sentiment_tweet'].apply(lambda x: 0 if sid.polarity_scores(x)['compound'] < 0.5 else 1)\n",
    "            df_twint = df_twint.rename(columns={'tweet': 'original_tweet', \\\n",
    "                                     'sentiment_tweet':'tweet'})[[\"id\",\"original_tweet\",\"tweet\", \"lat\", \"lng\", \"place\", \"date\", \"sentiment\"]]\n",
    "            df = pd.concat([df, df_twint])\n",
    "            df_twint.to_sql('tweet', con=engine, if_exists='append', index=False, chunksize = 1000, method=insert_on_duplicate)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77fbfce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_twint.tweet.apply(lambda x: clean_text(x))\n",
    "aaa = df.drop_duplicates('tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da238c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>place</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478190197552549890</td>\n",
       "      <td>@david_lunt @FlickReynolds I don't think they ...</td>\n",
       "      <td>dont think still using telephone appointments ...</td>\n",
       "      <td>-37.8136</td>\n",
       "      <td>144.9631</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-01-04 13:23:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478245717319634945</td>\n",
       "      <td>@SpaceKidette I read that the govt removed the...</td>\n",
       "      <td>read govt removed medicare rebate asymptomatic...</td>\n",
       "      <td>-37.8136</td>\n",
       "      <td>144.9631</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-01-04 17:03:38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478278968742539271</td>\n",
       "      <td>@abcnews Itâ€™s not free!  We pay taxes and and ...</td>\n",
       "      <td>free pay taxes medicare levy lied</td>\n",
       "      <td>-37.8136</td>\n",
       "      <td>144.9631</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-01-04 19:15:45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1478282700167413760</td>\n",
       "      <td>Remember in 2016 when Labor talked about Medic...</td>\n",
       "      <td>remember labor talked medicare liberals called...</td>\n",
       "      <td>-37.8136</td>\n",
       "      <td>144.9631</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-01-04 19:30:35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1478292232398512129</td>\n",
       "      <td>If she had the certificate &amp;amp; a Medicare ca...</td>\n",
       "      <td>ceificate amp medicare card cannot refuse entry</td>\n",
       "      <td>-37.8136</td>\n",
       "      <td>144.9631</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-01-04 20:08:28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1483625384847736832</td>\n",
       "      <td>@beccarala @bruce_haigh @sjb_one Well get your...</td>\n",
       "      <td>well get vaccines least wont die</td>\n",
       "      <td>-12.4381</td>\n",
       "      <td>130.8411</td>\n",
       "      <td>Northern Territory</td>\n",
       "      <td>2022-01-19 13:20:30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>1491012759718629376</td>\n",
       "      <td>@goodfoodgal Many vaccines can cause myocardit...</td>\n",
       "      <td>many vaccines cause myocarditis pericarditis u...</td>\n",
       "      <td>-37.5500</td>\n",
       "      <td>143.8500</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-02-08 22:35:18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>1490994675783987202</td>\n",
       "      <td>@truthvsagenda Search Dr Faucie a video where ...</td>\n",
       "      <td>search dr faucie video says vaccine gave made ...</td>\n",
       "      <td>-37.5500</td>\n",
       "      <td>143.8500</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-02-08 21:23:26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>1489338954729943042</td>\n",
       "      <td>@PriestessCeltic @secretpourpro @i_huffman91 S...</td>\n",
       "      <td>live lifeof hermit fear virus kills take vacci...</td>\n",
       "      <td>-37.5500</td>\n",
       "      <td>143.8500</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-02-04 07:44:12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1494004452864323584</td>\n",
       "      <td>@alisonmah @GetBAC But we were told it's the p...</td>\n",
       "      <td>told protesters violent apparently seems love ...</td>\n",
       "      <td>-36.7500</td>\n",
       "      <td>144.2667</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2022-02-17 04:43:13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33676 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                     original_tweet  \\\n",
       "0     1478190197552549890  @david_lunt @FlickReynolds I don't think they ...   \n",
       "1     1478245717319634945  @SpaceKidette I read that the govt removed the...   \n",
       "2     1478278968742539271  @abcnews Itâ€™s not free!  We pay taxes and and ...   \n",
       "3     1478282700167413760  Remember in 2016 when Labor talked about Medic...   \n",
       "4     1478292232398512129  If she had the certificate &amp; a Medicare ca...   \n",
       "...                   ...                                                ...   \n",
       "25    1483625384847736832  @beccarala @bruce_haigh @sjb_one Well get your...   \n",
       "2817  1491012759718629376  @goodfoodgal Many vaccines can cause myocardit...   \n",
       "2819  1490994675783987202  @truthvsagenda Search Dr Faucie a video where ...   \n",
       "2840  1489338954729943042  @PriestessCeltic @secretpourpro @i_huffman91 S...   \n",
       "0     1494004452864323584  @alisonmah @GetBAC But we were told it's the p...   \n",
       "\n",
       "                                                  tweet      lat       lng  \\\n",
       "0     dont think still using telephone appointments ... -37.8136  144.9631   \n",
       "1     read govt removed medicare rebate asymptomatic... -37.8136  144.9631   \n",
       "2                     free pay taxes medicare levy lied -37.8136  144.9631   \n",
       "3     remember labor talked medicare liberals called... -37.8136  144.9631   \n",
       "4       ceificate amp medicare card cannot refuse entry -37.8136  144.9631   \n",
       "...                                                 ...      ...       ...   \n",
       "25                     well get vaccines least wont die -12.4381  130.8411   \n",
       "2817  many vaccines cause myocarditis pericarditis u... -37.5500  143.8500   \n",
       "2819  search dr faucie video says vaccine gave made ... -37.5500  143.8500   \n",
       "2840  live lifeof hermit fear virus kills take vacci... -37.5500  143.8500   \n",
       "0     told protesters violent apparently seems love ... -36.7500  144.2667   \n",
       "\n",
       "                   place                 date  sentiment  \n",
       "0               Victoria  2022-01-04 13:23:01          0  \n",
       "1               Victoria  2022-01-04 17:03:38          0  \n",
       "2               Victoria  2022-01-04 19:15:45          0  \n",
       "3               Victoria  2022-01-04 19:30:35          0  \n",
       "4               Victoria  2022-01-04 20:08:28          0  \n",
       "...                  ...                  ...        ...  \n",
       "25    Northern Territory  2022-01-19 13:20:30          1  \n",
       "2817            Victoria  2022-02-08 22:35:18          0  \n",
       "2819            Victoria  2022-02-08 21:23:26          0  \n",
       "2840            Victoria  2022-02-04 07:44:12          0  \n",
       "0               Victoria  2022-02-17 04:43:13          0  \n",
       "\n",
       "[33676 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ddf170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(by=['place']).sum()[['sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df.date )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f648590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_df = df.groupby(by=['place', 'sentiment']).count() / df.groupby(by=['place']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a02817",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df = state_df[['date', 'id', 'lat', 'lng', 'original_tweet', 'tweet']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b089768",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df[state_df['sentiment']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579cd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35933a4c",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab8c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install geopandas fiona\n",
    "# !pip install shapely\n",
    "# !conda install geopandas\n",
    "# import fiona\n",
    "# !pip install matplotlib\n",
    "# !pip install fiona\n",
    "# !pip uninstall geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2d88d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import fiona\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08880ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file(\"https://raw.githubusercontent.com/tonywr71/GeoJson-Data/master/australian-states.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c704cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Display using geopandas\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,20))\n",
    "divider = make_axes_locatable(ax)\n",
    "tmp = state_df[state_df['sentiment']==1].copy()\n",
    "tmp['tweet'] = tmp['tweet']*100 #To display percentages\n",
    "cax = divider.append_axes(\"right\", size=\"3%\", pad=-1) #resize the colorbar\n",
    "tmp.plot(column='tweet', ax=ax,cax=cax,  legend=True, legend_kwds={'label': \"Unemployment rate\"})\n",
    "tmp.geometry.boundary.plot(color='#BABABA', ax=ax, linewidth=0.3) #Add some borders to the geometries\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0528905",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e703a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53f025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffac7301cfee45af025002f6195af1c02947d974ea9017d3adc5be678d21b34b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
